{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMDjSW8QJghRo685VzC+7Lx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JvIS87xd3ZC-","executionInfo":{"status":"ok","timestamp":1771336262001,"user_tz":-240,"elapsed":27460,"user":{"displayName":"Karthika A","userId":"13081220987821586739"}},"outputId":"0154decf-4449-46ea-953e-f428bc9d4f3e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torchvision.models import resnet50, ResNet50_Weights\n","from torch.utils.data import DataLoader, Subset\n","from sklearn.metrics import classification_report, confusion_matrix\n","from PIL import Image\n","from collections import defaultdict\n","import random\n","import os\n","\n","ROOT = \"/content/drive/MyDrive/warehouse_ai\"\n","\n","# Device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# Load Pretrained Weights\n","weights = ResNet50_Weights.DEFAULT\n","\n","# Transforms\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(30),\n","    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n","    transforms.RandomAffine(degrees=0, translate=(0.1,0.1)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=weights.transforms().mean,\n","                         std=weights.transforms().std)\n","])\n","\n","test_transform = weights.transforms()\n","\n","# Dataset\n","data_path = os.path.join(ROOT, \"ml_module\", \"dataset\")\n","full_dataset = datasets.ImageFolder(data_path)\n","\n","# Train/test split\n","class_to_indices = defaultdict(list)\n","for idx, (_, label) in enumerate(full_dataset.imgs):\n","    class_to_indices[label].append(idx)\n","\n","test_indices = []\n","train_indices = []\n","\n","for cls, indices in class_to_indices.items():\n","    random.shuffle(indices)\n","    n_test = max(1, int(0.2 * len(indices)))\n","    test_indices.extend(indices[:n_test])\n","    train_indices.extend(indices[n_test:])\n","\n","train_dataset = Subset(full_dataset, train_indices)\n","test_dataset = Subset(full_dataset, test_indices)\n","\n","train_dataset.dataset.transform = train_transform\n","test_dataset.dataset.transform = test_transform\n","\n","# DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n","\n","# Model\n","model = resnet50(weights=weights)\n","\n","# Fine-tune last ResNet block\n","for name, param in model.named_parameters():\n","    if \"layer4\" in name or \"fc\" in name:\n","        param.requires_grad = True\n","    else:\n","        param.requires_grad = False\n","\n","num_features = model.fc.in_features\n","model.fc = nn.Linear(num_features, 3)\n","model = model.to(device)\n","\n","# Loss & Optimizer\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0005)\n","\n","# Training\n","epochs = 10\n","for epoch in range(epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = loss_function(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","    running_loss /= len(train_loader)\n","    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss:.4f}\")\n","\n","print(\"Training Finished.\")\n","\n","# Evaluation\n","model.eval()\n","all_labels = []\n","all_preds = []\n","\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs, 1)\n","        all_labels.extend(labels.cpu().numpy())\n","        all_preds.extend(predicted.cpu().numpy())\n","\n","accuracy = 100 * (torch.tensor(all_preds) == torch.tensor(all_labels)).sum().item() / len(all_labels)\n","print(f\"\\nTest Accuracy: {accuracy:.2f}%\\n\")\n","print(\"Classification Report:\")\n","print(classification_report(all_labels, all_preds, target_names=full_dataset.classes))\n","print(\"Confusion Matrix:\")\n","print(confusion_matrix(all_labels, all_preds))\n","\n","# Inference demo\n","img_path = os.path.join(ROOT, \"ml_module\", \"dataset\", \"fragile\", \"000002.jpg\")\n","img = Image.open(img_path).convert(\"RGB\")\n","img_tensor = test_transform(img).unsqueeze(0).to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    output = model(img_tensor)\n","    _, pred = torch.max(output, 1)\n","\n","print(f\"\\nImage: {img_path}\")\n","print(f\"Predicted class: {full_dataset.classes[pred.item()]}\")\n","\n","# Save Model\n","save_path = os.path.join(ROOT, \"ml_module\", \"resnet50_warehouse.pth\")\n","torch.save(model.state_dict(), save_path)\n","print(f\"Model saved successfully at {save_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4sGaBtcW_0CB","executionInfo":{"status":"ok","timestamp":1771338528190,"user_tz":-240,"elapsed":105789,"user":{"displayName":"Karthika A","userId":"13081220987821586739"}},"outputId":"368e843d-1ccc-41b0-db2e-563318be6f12"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Epoch [1/10], Loss: 1.0980\n","Epoch [2/10], Loss: 0.6202\n","Epoch [3/10], Loss: 0.3613\n","Epoch [4/10], Loss: 0.1908\n","Epoch [5/10], Loss: 0.0644\n","Epoch [6/10], Loss: 0.0545\n","Epoch [7/10], Loss: 0.0241\n","Epoch [8/10], Loss: 0.0670\n","Epoch [9/10], Loss: 0.0337\n","Epoch [10/10], Loss: 0.0117\n","Training Finished.\n","\n","Test Accuracy: 83.33%\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","     fragile       0.67      1.00      0.80         2\n","   hazardous       1.00      1.00      1.00         2\n","       heavy       1.00      0.50      0.67         2\n","\n","    accuracy                           0.83         6\n","   macro avg       0.89      0.83      0.82         6\n","weighted avg       0.89      0.83      0.82         6\n","\n","Confusion Matrix:\n","[[2 0 0]\n"," [0 2 0]\n"," [1 0 1]]\n","\n","Image: /content/drive/MyDrive/warehouse_ai/ml_module/dataset/fragile/000002.jpg\n","Predicted class: fragile\n","Model saved successfully at /content/drive/MyDrive/warehouse_ai/ml_module/resnet50_warehouse.pth\n"]}]},{"cell_type":"markdown","source":["#Limitations of the Machine Learning Model\n","\n","The main limitation of this machine learning model is the **small size of the dataset**, which contains only a few images per class. While data augmentation increases diversity, the model may still struggle with real-world variations in object appearance, lighting, and orientation. As a result, test metrics can be highly sensitive: a single misclassification significantly impacts accuracy, precision, and recall.\n","\n","Additionally, the model is trained using only **static images**. In a real warehouse scenario, objects may appear at different distances, under occlusion, or partially visible, which may reduce the model’s reliability.\n","\n","Finally, only the **last ResNet block is fine-tuned** to prevent overfitting. While this approach works well for small datasets, it may limit the model’s ability to fully adapt to warehouse-specific features, such as complex textures or mixed materials.\n","\n","Advantage: Despite these limitations, the model effectively demonstrates classification capabilities with **high accuracy** and **reliable inference** on new images."],"metadata":{"id":"eQS4uw0LGVSe"}}]}